{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "optional-australia",
   "metadata": {},
   "source": [
    "# Pretrain BigBird on GENCODE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "signed-provincial",
   "metadata": {},
   "source": [
    "This notebook uses the `transformers` library to pretrain the BigBird architecture on GENCODE using MLM loss. Adapted from Hugging Face script https://github.com/huggingface/transformers/blob/master/examples/legacy/run_language_modeling.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assured-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import AutoConfig\n",
    "from transformers import BigBirdForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from transformers import LineByLineTextDataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cutting-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov  5 18:09:55 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla K80           On   | 00000001:00:00.0 Off |                    0 |\r\n",
      "| N/A   40C    P8    29W / 149W |      0MiB / 11441MiB |      0%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|  No running processes found                                                 |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "commercial-alexander",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-scholarship",
   "metadata": {},
   "source": [
    "## Train the tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-booth",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"./data/6mers_train_tiny.txt\", \"./data/6mers_test_tiny.txt\"]\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# Customize training\n",
    "trainer = WordLevelTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "tokenizer.train(files=paths, trainer=trainer)\n",
    "\n",
    "# Convert to PretrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast(\n",
    "    tokenizer_object=tokenizer,\n",
    "    unk_token=\"[UNK]\",\n",
    "    cls_token=\"[CLS]\",\n",
    "    sep_token=\"[SEP]\",\n",
    "    pad_token=\"[PAD]\",\n",
    "    mask_token=\"[MASK]\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elementary-flower",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1524"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.get_vocab())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-retail",
   "metadata": {},
   "source": [
    "## Train language model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "liable-anime",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"google/bigbird-roberta-base\"\n",
    "config = AutoConfig.from_pretrained(model_checkpoint)\n",
    "model = BigBirdForMaskedLM(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "verbal-vulnerability",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128111286"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.num_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rapid-liverpool",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "excited-paint",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/api_env_dev/lib/python3.7/site-packages/transformers/data/datasets/language_modeling.py:124: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "train_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=paths[0],\n",
    "    block_size=128,\n",
    ")\n",
    "\n",
    "eval_dataset = LineByLineTextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=paths[1],\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "listed-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_dir = \"./output_model\"\n",
    "output_tokenizer_dir = \"./output_tokenizer\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_model_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_gpu_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "powered-stake",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "***** Running training *****\n",
      "  Num examples = 4\n",
      "  Num Epochs = 50\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 50\n",
      "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
      "Attention type 'block_sparse' is not possible if sequence_length: 128 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:16, Epoch 50/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric train_runtime:\n",
      "17.1217\n",
      "Attempted to log scalar metric train_samples_per_second:\n",
      "11.681\n",
      "Attempted to log scalar metric train_steps_per_second:\n",
      "2.92\n",
      "Attempted to log scalar metric total_flos:\n",
      "13254002380800.0\n",
      "Attempted to log scalar metric train_loss:\n",
      "9.804313354492187\n",
      "Attempted to log scalar metric epoch:\n",
      "50.0\n",
      "CPU times: user 15.5 s, sys: 1.58 s, total: 17.1 s\n",
      "Wall time: 17.1 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=50, training_loss=9.804313354492187, metrics={'train_runtime': 17.1217, 'train_samples_per_second': 11.681, 'train_steps_per_second': 2.92, 'total_flos': 13254002380800.0, 'train_loss': 9.804313354492187, 'epoch': 50.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "technological-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./output_model\n",
      "Configuration saved in ./output_model/config.json\n",
      "Model weights saved in ./output_model/pytorch_model.bin\n",
      "tokenizer config file saved in ./output_tokenizer/tokenizer_config.json\n",
      "Special tokens file saved in ./output_tokenizer/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./output_tokenizer/tokenizer_config.json',\n",
       " './output_tokenizer/special_tokens_map.json',\n",
       " './output_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(output_model_dir)\n",
    "tokenizer.save_pretrained(output_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-institution",
   "metadata": {},
   "source": [
    "# Validate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "professional-shelter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempted to log scalar metric eval_loss:\n",
      "10.796552658081055\n",
      "Attempted to log scalar metric eval_runtime:\n",
      "0.0958\n",
      "Attempted to log scalar metric eval_samples_per_second:\n",
      "41.763\n",
      "Attempted to log scalar metric eval_steps_per_second:\n",
      "10.441\n",
      "Attempted to log scalar metric epoch:\n",
      "50.0\n",
      "Perplexity: 48852.10\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lesbian-yacht",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./output_model/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "loading configuration file ./output_model/config.json\n",
      "Model config BigBirdConfig {\n",
      "  \"architectures\": [\n",
      "    \"BigBirdForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_type\": \"block_sparse\",\n",
      "  \"block_size\": 64,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu_new\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 4096,\n",
      "  \"model_type\": \"big_bird\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_random_blocks\": 3,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"rescale_embeddings\": false,\n",
      "  \"sep_token_id\": 66,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.9.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bias\": true,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50358\n",
      "}\n",
      "\n",
      "loading weights file ./output_model/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BigBirdForMaskedLM.\n",
      "\n",
      "All the weights of BigBirdForMaskedLM were initialized from the model checkpoint at ./output_model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BigBirdForMaskedLM for predictions without further training.\n",
      "Didn't find file ./output_tokenizer/added_tokens.json. We won't load it.\n",
      "loading file None\n",
      "loading file ./output_tokenizer/special_tokens_map.json\n",
      "loading file ./output_tokenizer/tokenizer_config.json\n",
      "loading file ./output_tokenizer/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=output_model_dir,\n",
    "    tokenizer=output_tokenizer_dir,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bacterial-passing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Attention type 'block_sparse' is not possible if sequence_length: 3 <= num global tokens: 2 * config.block_size + min. num sliding tokens: 3 * config.block_size + config.num_random_blocks * config.block_size + additional buffer: config.num_random_blocks * config.block_size = 704 with config.block_size = 64, config.num_random_blocks = 3.Changing attention type to 'original_full'...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'sequence': 'TTAGTT TAGTTT TGGAAG',\n",
       "  'score': 0.0002077565441140905,\n",
       "  'token': 13,\n",
       "  'token_str': 'TGGAAG'},\n",
       " {'sequence': 'TTAGTT TAGTTT AGGAAC',\n",
       "  'score': 0.0001975743507500738,\n",
       "  'token': 1111,\n",
       "  'token_str': 'AGGAAC'},\n",
       " {'sequence': 'TTAGTT TAGTTT GTGGCC',\n",
       "  'score': 0.00018981598259415478,\n",
       "  'token': 796,\n",
       "  'token_str': 'GTGGCC'},\n",
       " {'sequence': 'TTAGTT TAGTTT TCTTCC',\n",
       "  'score': 0.00018416137027088553,\n",
       "  'token': 172,\n",
       "  'token_str': 'TCTTCC'},\n",
       " {'sequence': 'TTAGTT TAGTTT TAAGAG',\n",
       "  'score': 0.00018064815958496183,\n",
       "  'token': 625,\n",
       "  'token_str': 'TAAGAG'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_mask(\"TTAGTT TAGTTT [MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closing-wagner",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "api_env_dev",
   "language": "python",
   "name": "api_env_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
